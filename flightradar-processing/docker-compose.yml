version: '3.8'
networks:
  kafka-logstash-spark-collector:
    external: true

services:
  spark:
    image: docker.io/bitnami/spark:latest
    container_name: spark_master
    restart: unless-stopped
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    networks:
      - kafka-logstash-spark-collector
    ports:
      - '38080:8080'
      - '37077:7077'
    volumes:
      - ./app:/app
      - ./spark-conf/spark-defaults.conf:/bitnami/spark/conf/spark-defaults.conf
  spark-worker:
    image: docker.io/bitnami/spark:latest
    container_name: spark_worker0
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=3G
      - SPARK_WORKER_CORES=3
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    networks:
      - kafka-logstash-spark-collector
    ports:
      - '38081:8081'
    volumes:
      - ./app:/app

  work-env:
    image: docker.io/bitnami/spark:latest
    container_name: exec-app
    volumes:
      - ./app:/app
    command: tail -f /dev/null
    user: root
    working_dir: /app
    networks:
      - kafka-logstash-spark-collector
    environment:
      PYTHONPATH: /opt/bitnami/spark/python:/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip 
